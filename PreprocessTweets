#.......Gerekli kütüphaneler import edilir
import nltk                                # Python library for NLP
from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK
import matplotlib.pyplot as plt            # library for visualization
import random                              # pseudo-random number generator

nltk.download('twitter_samples')

#........Pozitif ve negatif tweet setleri seçilir
all_positive_tweets = twitter_samples.strings('positive_tweets.json')
all_negative_tweets = twitter_samples.strings('negative_tweets.json')

#........

print('Number of positive tweets: ', len(all_positive_tweets))
print('Number of negative tweets: ', len(all_negative_tweets))

print('\nThe type of all_positive_tweets is: ', type(all_positive_tweets))
print('The type of a tweet entry is: ', type(all_negative_tweets[0]))

#........
# Grafik için boyut belirlenir
fig = plt.figure(figsize=(5, 5))

# İki sınıf için etiketler
labels = 'Positives', 'Negative'

# Her kısım için boyutlar
sizes = [len(all_positive_tweets), len(all_negative_tweets)] 

# Pasta grafik oluşturulur
plt.pie(sizes, labels=labels, autopct='%1.1f%%',
        shadow=True, startangle=90)

# Eşit en boy oranı seçilerek daire şeklinde bir pasta grafik oluşruracağız
plt.axis('equal')  

# Grafiği görelim
plt.show()


#........
#Verileri anlamak veri bilimi projelerindeki başarı veya başarısızlığın % 80'inden sorumludur.
#Yani veriyi inceleyelim

# Pozitif tweetlerde iken yeşil yazıya geçelim
print('\033[92m' + all_positive_tweets[random.randint(0,5000)])

# Negatif tweetlerde iken kırmızı yazıya geçelim
print('\033[91m' + all_negative_tweets[random.randint(0,5000)])


#.........
# Bir örnek tweet
tweet = all_positive_tweets[2277]
print(tweet)

#.........
# download the stopwords from NLTK
nltk.download('stopwords')

#.........
import re                                  # library for regular expression operations
import string                              # for string operations

from nltk.corpus import stopwords          # module for stop words that come with NLTK
from nltk.stem import PorterStemmer        # module for stemming
from nltk.tokenize import TweetTokenizer   # module for tokenizing strings

#.........
print('\033[92m' + tweet)
print('\033[94m')

# eski usul retweet kelimesini sil "RT"
tweet2 = re.sub(r'^RT[\s]+', '', tweet)

# hyperlinkkleri sil
tweet2 = re.sub(r'https?:\/\/.*[\r\n]*', '', tweet2)

# hashtagleri sil
# only removing the hash # sign from the word
tweet2 = re.sub(r'#', '', tweet2)

print(tweet2)

#...........
print()
print('\033[92m' + tweet2)
print('\033[94m')

# tokenizer class nesnesi oluştur
tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,
                               reduce_len=True)

# tweetleri ayır
tweet_tokens = tokenizer.tokenize(tweet2)

print()
print('Tokenized string:')
print(tweet_tokens)

#........
# NLTK den ıngılızce stopwords indir
stopwords_english = stopwords.words('english') 

print('Stop words\n')
print(stopwords_english)

print('\nPunctuation\n')
print(string.punctuation)

#.........
print()
print('\033[92m')
print(tweet_tokens)
print('\033[94m')

tweets_clean = []

for word in tweet_tokens: # Go through every word in your tokens list
    if (word not in stopwords_english and  # remove stopwords
        word not in string.punctuation):  # remove punctuation
        tweets_clean.append(word)

print('removed stop words and punctuation:')
print(tweets_clean)

#............
print()
print('\033[92m')
print(tweets_clean)
print('\033[94m')

# Instantiate stemming class
stemmer = PorterStemmer() 

# Create an empty list to store the stems
tweets_stem = [] 

for word in tweets_clean:
    stem_word = stemmer.stem(word)  # stemming word
    tweets_stem.append(stem_word)  # append to the list

print('stemmed words:')
print(tweets_stem)

